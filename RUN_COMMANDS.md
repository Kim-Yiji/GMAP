# ğŸš€ ì„œë²„ì—ì„œ ë°”ë¡œ ì‹¤í–‰í•˜ëŠ” ëª…ë ¹ì–´ ëª¨ìŒ

> ì„œë²„ í™˜ê²½ì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆê³  SSHê°€ ì—°ê²°ëœ ìƒíƒœì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª…ë ¹ì–´ë“¤

## âš¡ ë¹ ë¥¸ ì‹¤í–‰ (Copy & Paste)

### 1. í”„ë¡œì íŠ¸ ì¤€ë¹„
```bash
# í”„ë¡œì íŠ¸ í´ë¡  (ìµœì´ˆ 1íšŒë§Œ)
git clone https://github.com/Kim-Yiji/Comjonsul.git
cd Comjonsul  
git checkout CYisSMART

# ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ 1íšŒë§Œ)
pip install einops  # ì¶”ê°€ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
```

### 2. ë¹ ë¥¸ ê²€ì¦
```bash
# Shape ê²€ì¦ í…ŒìŠ¤íŠ¸
python demo_final.py
```

## ğŸš€ **Data Caching (ì‹ ê¸°ëŠ¥!)**

**ì²« ë²ˆì§¸ ì‹¤í–‰**: ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ìºì‹œ ìƒì„± (2-5ë¶„)  
**ì´í›„ ì‹¤í–‰**: ìºì‹œì—ì„œ ë°”ë¡œ ë¡œë”© **(5-10ë°° ë¹ ë¦„!)** âš¡

```bash
# ìë™ ìºì‹± (ê¸°ë³¸ í™œì„±í™”)
python train_unified.py --dataset eth --batch_size 8

# ìºì‹± ë¹„í™œì„±í™” (ì²« ì‹¤í–‰ì´ë‚˜ ë””ë²„ê¹… ì‹œ)
python train_unified.py --dataset eth --no-use_cache

# ì»¤ìŠ¤í…€ ìºì‹œ ë””ë ‰í† ë¦¬
python train_unified.py --dataset eth --cache_dir ./my_cache
```

## ğŸ“ˆ í•™ìŠµ ì‹¤í–‰

#### ğŸƒâ€â™‚ï¸ ê¸°ë³¸ í•™ìŠµ
```bash
# ETH ë°ì´í„°ì…‹ í•™ìŠµ (50 ì—í¬í¬)
python train_unified.py \
    --dataset eth \
    --batch_size 8 \
    --num_epochs 50 \
    --lr 1e-4 \
    --obs_len 8 \
    --pred_len 12 \
    --tag "quick_experiment"
```

#### ğŸš€ ê³ ì„±ëŠ¥ í•™ìŠµ
```bash
# ë” í° ëª¨ë¸ë¡œ í•™ìŠµ
python train_unified.py \
    --dataset eth \
    --batch_size 16 \
    --num_epochs 100 \
    --lr 1e-4 \
    --d_h 256 \
    --d_gp_in 256 \
    --dmrgcn_hidden_dims 128 128 256 256 \
    --agg_method gru \
    --mix_type attention \
    --tag "large_model"
```

#### ğŸŒ™ ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ (SSH ëŠì–´ì ¸ë„ ê³„ì†)
```bash
# nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python train_unified.py \
    --dataset eth \
    --batch_size 8 \
    --num_epochs 100 \
    --lr 1e-4 \
    --tag "background_training" > training.log 2>&1 &

# ì§„í–‰ìƒí™© í™•ì¸
tail -f training.log
```

### 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
python test_unified.py \
    --dataset eth \
    --model_path ./checkpoints_unified/quick_experiment-eth/eth_best.pth \
    --obs_len 8 \
    --pred_len 12
```

### 5. ëª¨ë‹ˆí„°ë§
```bash
# GPU ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ í™•ì¸
watch -n 1 nvidia-smi

# í•™ìŠµ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸  
tail -f training.log

# ì‹¤í–‰ ì¤‘ì¸ Python í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep python
```

## ğŸ§ª ë‹¤ì–‘í•œ ì‹¤í—˜

### Ablation Study
```bash
# Agent pathë§Œ ì‚¬ìš©
python train_unified.py \
    --dataset eth \
    --enable_agent --disable_intra --disable_inter \
    --tag "agent_only"

# Simple head vs GP-Graph head ë¹„êµ
python train_unified.py --use_simple_head --tag "simple_head"
python train_unified.py --tag "gpgraph_head"
```

### ë‹¤ë¥¸ ë°ì´í„°ì…‹
```bash
# Hotel ë°ì´í„°ì…‹
python train_unified.py --dataset hotel --tag "hotel_exp"

# ëª¨ë“  ë°ì´í„°ì…‹ ìˆœì°¨ í•™ìŠµ
for dataset in eth hotel univ zara1 zara2; do
    python train_unified.py --dataset $dataset --num_epochs 30 --tag "multi_dataset_$dataset"
done
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```bash
# í•™ìŠµë¥  ë³€ê²½
python train_unified.py --lr 1e-3 --tag "high_lr"
python train_unified.py --lr 5e-5 --tag "low_lr"

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì‹¤í—˜
python train_unified.py --batch_size 32 --tag "large_batch"
python train_unified.py --batch_size 4 --tag "small_batch"
```

## ğŸ’¾ **Cache Management**

### ìºì‹œ ìƒíƒœ í™•ì¸
```bash
# ìºì‹œ í¬ê¸° í™•ì¸
du -sh ./data_cache/*/

# ìºì‹œ íŒŒì¼ ëª©ë¡
ls -la ./data_cache/*/
```

### ìºì‹œ ê´€ë¦¬
```bash
# ì „ì²´ ìºì‹œ ì‚­ì œ (ì²« ì‹¤í–‰ì²˜ëŸ¼ ì²˜ë¦¬)
rm -rf ./data_cache/

# íŠ¹ì • ë°ì´í„°ì…‹ ìºì‹œë§Œ ì‚­ì œ
rm -rf ./data_cache/eth/

# ìºì‹œ ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
find ./data_cache -name "*.pkl" -exec ls -lh {} \;
```

## ğŸ“Š ê²°ê³¼ ë¶„ì„

### ì²´í¬í¬ì¸íŠ¸ í™•ì¸
```bash
# ì €ì¥ëœ ëª¨ë¸ë“¤ í™•ì¸
ls -la checkpoints_unified/*/

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
find checkpoints_unified/ -name "*_best.pth"
```

### TensorBoard
```bash
# TensorBoard ì‹¤í–‰ (í¬íŠ¸ 6006)
tensorboard --logdir ./checkpoints_unified --port 6006

# ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
nohup tensorboard --logdir ./checkpoints_unified --port 6006 > tensorboard.log 2>&1 &
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```bash
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
python train_unified.py --batch_size 4 --tag "small_memory"

# ì‘ì€ ëª¨ë¸ ì‚¬ìš©
python train_unified.py \
    --d_h 64 \
    --dmrgcn_hidden_dims 32 32 64 \
    --tag "lightweight"
```

### í•™ìŠµ ì¤‘ë‹¨/ì¬ì‹œì‘
```bash
# ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
kill $(ps aux | grep "train_unified.py" | awk '{print $2}' | head -1)

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
python train_unified.py \
    --resume ./checkpoints_unified/quick_experiment-eth/checkpoint_epoch_20.pth \
    --tag "resumed_training"
```

## ğŸ“‹ ì¶”ì²œ ì‹¤í–‰ ìˆœì„œ

1. **ë¹ ë¥¸ ê²€ì¦**: `python demo_final.py`
2. **ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸**: `python train_unified.py --dataset eth --num_epochs 10 --tag "test"`
3. **ë³¸ê²© í•™ìŠµ**: ìœ„ì˜ ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ëª…ë ¹ì–´ ì‚¬ìš©
4. **ì„±ëŠ¥ í‰ê°€**: `python test_unified.py` ëª…ë ¹ì–´ ì‚¬ìš©

---

## ğŸ’¡ **Performance Tips**

### ğŸš€ **First Run vs Cached Run**
```
ğŸ“Š Typical Performance:
   First Run:  2-5 minutes (data processing)
   Cached Run: 10-30 seconds (5-10x faster!)
   
ğŸ’¾ Cache Storage: ~50-200MB per dataset
ğŸ“ Cache Location: ./data_cache/{dataset}/
```

### âš¡ **Best Practices**
1. **Keep Cache**: ìºì‹œë¥¼ ì‚­ì œí•˜ì§€ ë§ˆì„¸ìš” (ë‹¤ìŒ ì‹¤í–‰ì´ ë¹¨ë¼ì§‘ë‹ˆë‹¤)
2. **Consistent Parameters**: ê°™ì€ `obs_len`/`pred_len` ì‚¬ìš©
3. **Monitor Storage**: ê°€ë” ìºì‹œ ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
4. **Server Benefits**: ëŠë¦° ìŠ¤í† ë¦¬ì§€ì—ì„œ íŠ¹íˆ ìœ ìš©

---

**ğŸ’¡ Tip**: ëª¨ë“  ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•´ì„œ í„°ë¯¸ë„ì— ë°”ë¡œ ë¶™ì—¬ë„£ê¸°í•˜ë©´ ë©ë‹ˆë‹¤!  
**ğŸ‰ Data Cachingìœ¼ë¡œ ì´ì œ í›¨ì”¬ ë¹ ë¥´ê²Œ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** âš¡
