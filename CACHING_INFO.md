# ğŸš€ Data Caching System

## ğŸ“‹ Overview

ë°ì´í„° ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ìºì‹±í•˜ì—¬ ë‘ ë²ˆì§¸ ì‹¤í–‰ë¶€í„°ëŠ” í›¨ì”¬ ë¹ ë¥´ê²Œ ë¡œë”©í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

## âœ¨ Key Features

### ğŸ”„ **Automatic Caching**
- **ì²« ë²ˆì§¸ ì‹¤í–‰**: ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ìºì‹œ íŒŒì¼ ìƒì„±
- **ì´í›„ ì‹¤í–‰**: ìºì‹œì—ì„œ ë°”ë¡œ ë¡œë”© (5-10ë°° ë¹ ë¦„)
- **ìë™ ê°ì§€**: ë°ì´í„°ë‚˜ íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ ì¬ì²˜ë¦¬

### ğŸ’¾ **Smart Cache Management**
- **Unique Keys**: íŒŒë¼ë¯¸í„°ì™€ íŒŒì¼ ë³€ê²½ì‹œê°„ ê¸°ë°˜ í•´ì‹œ
- **Invalidation**: ë°ì´í„° ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ ìºì‹œ ë¬´íš¨í™”
- **Compression**: Pickleì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ì €ì¥

### ğŸ¯ **What Gets Cached**
- **Raw Trajectories**: `obs_traj`, `pred_traj`, `obs_traj_rel`, `pred_traj_rel`
- **Masks**: `loss_mask`, `non_linear_ped`
- **Agent IDs**: `agent_ids_list`
- **Graph Data**: `V_obs`, `A_obs`, `V_pred`, `A_pred` (ê°€ì¥ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ë¶€ë¶„)

## ğŸš€ Usage

### ê¸°ë³¸ ì‚¬ìš©ë²• (ìë™ í™œì„±í™”)
```python
# train_unified.pyì—ì„œ ìë™ìœ¼ë¡œ ìºì‹± ì‚¬ìš©
python train_unified.py --dataset eth --batch_size 8 --num_epochs 50
```

### ìºì‹± ì˜µì…˜ ì œì–´
```bash
# ìºì‹± ë¹„í™œì„±í™”
python train_unified.py --dataset eth --no-use_cache

# ì»¤ìŠ¤í…€ ìºì‹œ ë””ë ‰í† ë¦¬
python train_unified.py --dataset eth --cache_dir ./my_custom_cache
```

### ë‹¤ì¤‘ ë°ì´í„°ì…‹ í•™ìŠµ
```bash
# ê° ë°ì´í„°ì…‹ë§ˆë‹¤ ë³„ë„ ìºì‹œ ìƒì„±
for dataset in eth hotel univ zara1 zara2; do
    python train_unified.py --dataset $dataset --num_epochs 30
done
```

## ğŸ“Š Performance Benefits

### ğŸ• **Time Savings**
- **First Run**: ~2-5 minutes (ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼)
- **Cached Run**: ~10-30 seconds
- **Speedup**: **5-10x faster** âš¡

### ğŸ’½ **Storage**
- **Cache Size**: ë³´í†µ 50-200MB per dataset
- **Location**: `./data_cache/{dataset_name}/`
- **Format**: `.pkl` files with unique hash names

## ğŸ”§ Technical Details

### Cache Key Generation
```python
# íŒŒë¼ë¯¸í„° + íŒŒì¼ ìˆ˜ì •ì‹œê°„ìœ¼ë¡œ ìœ ë‹ˆí¬ í‚¤ ìƒì„±
params = f"{dataset}_{obs_len}_{pred_len}_{skip}_{delim}"
file_info = [(filename, modification_time), ...]
cache_key = md5(params + file_info).hexdigest()[:16]
```

### Cache Structure
```
data_cache/
â”œâ”€â”€ eth/
â”‚   â”œâ”€â”€ a1b2c3d4e5f6g7h8.pkl  # train data cache
â”‚   â””â”€â”€ f9e8d7c6b5a4321.pkl   # val data cache
â”œâ”€â”€ hotel/
â”‚   â”œâ”€â”€ x1y2z3w4v5u6t7s8.pkl
â”‚   â””â”€â”€ s9r8q7p6o5n4m321.pkl
â””â”€â”€ ...
```

## âš™ï¸ Configuration

### Arguments in `train_unified.py`
```python
parser.add_argument('--use_cache', action='store_true', default=True,
                   help='Use cached preprocessed data')
parser.add_argument('--cache_dir', default='./data_cache',
                   help='Directory for data cache')
```

### TrajectoryDataset Parameters
```python
TrajectoryDataset(
    data_dir='./copy_dmrgcn/datasets/eth/train/',
    obs_len=8,
    pred_len=12,
    skip=1,
    min_ped=1,
    delim='tab',
    use_cache=True,           # Enable/disable caching
    cache_dir='./data_cache'  # Cache directory
)
```

## ğŸ” Cache Validation

### When Cache is Used
âœ… Same dataset, parameters, and file modification times
âœ… Cache file exists and is readable
âœ… All required data fields are present

### When Cache is Invalidated
âŒ Different `obs_len`, `pred_len`, `skip`, or other parameters
âŒ Dataset files have been modified
âŒ Cache file is corrupted or missing fields
âŒ Different delimiter or min_ped settings

## ğŸ§¹ Cache Management

### Clear All Caches
```bash
rm -rf ./data_cache/
```

### Clear Specific Dataset Cache
```bash
rm -rf ./data_cache/eth/
```

### Check Cache Status
```bash
# See cache sizes
du -sh ./data_cache/*/

# List cache files
find ./data_cache -name "*.pkl" -exec ls -lh {} \;
```

## ğŸš¨ Troubleshooting

### "Processing data from scratch" Every Time
- Check if file modification times are changing
- Verify parameters are consistent
- Ensure cache directory has write permissions

### Large Cache Files
- Normal for large datasets (ETH ~100MB, Hotel ~50MB)
- Cache includes preprocessed graph adjacency matrices
- Trade-off: storage space for computation time

### Memory Issues
- Cache loading requires sufficient RAM
- Large datasets may need 1-2GB for cache loading
- Consider smaller batch sizes if memory limited

## ğŸ’¡ Best Practices

1. **Keep Cache**: Don't delete cache between training runs
2. **Consistent Params**: Use same obs_len/pred_len for experiments
3. **Monitor Storage**: Check cache directory size periodically
4. **Server Usage**: Cache is especially beneficial on slow storage systems

## ğŸ¯ Benefits for Your Workflow

### Development
- **Rapid Iteration**: Quick model parameter changes
- **Debug Friendly**: Fast data loading for debugging
- **Experiment Speed**: Multiple runs without data preprocessing delay

### Production
- **Training Efficiency**: Start training immediately
- **Resource Savings**: Less CPU usage for data preprocessing
- **Reproducibility**: Consistent data processing across runs

---

**ğŸ‰ Now you can run experiments much faster!** The first run creates the cache, and every subsequent run with the same parameters will be lightning fast! âš¡
